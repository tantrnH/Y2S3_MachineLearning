{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EOY6hMPyOn4-"
      },
      "outputs": [],
      "source": [
        "# import the required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Gridworld Environment\n",
        "class Gridworld:\n",
        "    def __init__(self, size=5, start=(0, 0), goal=(4, 4), obstacles=[(2, 2), (3, 3)]):\n",
        "        self.size = size # size of grid\n",
        "        self.start = start # starting position of the agent\n",
        "        self.goal = goal # goal position that the agent aims to reach\n",
        "        self.obstacles = obstacles # List of grid postions that the agent can't move\n",
        "        self.state = start\n",
        "\n",
        "    def reset(self): # reset the environment to the initial state\n",
        "        self.state = self.start # store the current postion of the agent\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state\n",
        "        if action == 0:  # Up\n",
        "            x = max(0, x - 1)\n",
        "\n",
        "        elif action == 1:  # Down\n",
        "            x = min(self.size - 1, x + 1)\n",
        "\n",
        "        elif action == 2:  # Left\n",
        "            y = max(0, y - 1)\n",
        "\n",
        "        elif action == 3:  # Right\n",
        "            y = min(self.size - 1, y + 1)\n",
        "\n",
        "        new_state = (x, y) # update the environment based on the agent's action.\n",
        "        if new_state in self.obstacles:\n",
        "\n",
        "            reward = -5  # Penalty for hitting obstacle ; [[ One reward can be one Function ]]\n",
        "            new_state = self.state  # Stay in the same place\n",
        "\n",
        "        elif new_state == self.goal:\n",
        "\n",
        "            reward = 10  # Reward for reaching the goal\n",
        "        else:\n",
        "            reward = -1  # Penalty for each step\n",
        "\n",
        "        self.state = new_state\n",
        "\n",
        "        done = new_state == self.goal\n",
        "        return new_state, reward, done\n",
        "\n",
        "    def get_state_space(self):\n",
        "        return [(i, j) for i in range(self.size) for j in range(self.size)]"
      ],
      "metadata": {
        "id": "eVjPIPmqO6fa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-learning Algorithm\n",
        "class QLearningAgent:\n",
        "    def __init__(self, env,\n",
        "                 alpha=0.1,\n",
        "                 gamma=0.9,\n",
        "                 epsilon=1.0,\n",
        "                 epsilon_decay=0.995,\n",
        "                 min_epsilon=0.01,\n",
        "                 episodes=1000):\n",
        "        self.env = env\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.episodes = episodes\n",
        "        self.q_table = self._initialize_q_table()\n",
        "\n",
        "    def _initialize_q_table(self):\n",
        "        states = self.env.get_state_space\n",
        "        return {state: np.zeros(4) for state in states}  # 4 actions (Up, Down, Left, Right)\n",
        "\n",
        "    def choose_action(self, state):                 #choose action\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(4)  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])  # Exploit best action\n",
        "\n",
        "    def train(self):                    # Do Training\n",
        "        rewards_per_episode = []\n",
        "        for episode in range(self.episodes):\n",
        "            state = self.env.reset\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done = self.env.step(action)\n",
        "\n",
        "                # Q-value update using Bellman equation\n",
        "                self.q_table[state][action] = self.q_table[state][action] + \\\n",
        "                    self.alpha * (reward + self.gamma * np.max(self.q_table[next_state]) - self.q_table[state][action])\n",
        "\n",
        "                state = next_state          # update the state to the next state\n",
        "                total_reward += reward      # it is a cumulative reward\n",
        "\n",
        "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "            rewards_per_episode.append(total_reward)\n",
        "\n",
        "        return rewards_per_episode\n",
        "\n",
        "    def get_policy(self):\n",
        "        policy = {}\n",
        "        for state in self.q_table:\n",
        "            policy[state] = np.argmax(self.q_table[state])\n",
        "        return policy\n"
      ],
      "metadata": {
        "id": "wHcq-NwvO6iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Evaluation\n",
        "env = ________\n",
        "agent = QLearningAgent(_________)\n",
        "rewards = __________\n",
        "policy = _________\n",
        "\n",
        "# Visualization: Training Progress\n",
        "plt.figure(figsize=______)\n",
        "plt.plot_________\n",
        "plt.xlabel(_________)\n",
        "plt.ylabel(_________)\n",
        "plt.title(_________)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_UpOeWYvO6mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization: Heatmap of Q-values\n",
        "q_values_matrix = __________\n",
        "for state, actions in agent.q_table.items():\n",
        "    q_values_matrix[state] = np.max(actions)\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "sns.heatmap(q_values_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(__________)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "US5kh2v0O6pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization: Policy Map\n",
        "action_symbols = _________\n",
        "policy_grid = np.full((env.size, env.size), \" \", dtype=str)\n",
        "for state, action in policy.items():\n",
        "    policy_grid[state] = action_symbols[action]"
      ],
      "metadata": {
        "id": "s8gZ8n66O6sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(_________)\n",
        "for _______\n",
        "    print(__________)"
      ],
      "metadata": {
        "id": "Jt3m9W18O6uY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}