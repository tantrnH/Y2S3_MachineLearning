{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "  Performs gradient descent to find optimal weights and bias for linear regression.\n",
        "\n",
        "  Args:\n",
        "\n",
        "  X: A numpy array of shape (m, n) representing the training data features.\n",
        "  \n",
        "  y: A numpy array of shape (m,) representing the training data target values.\n",
        "  \n",
        "  learning_rate: The learning rate to control the step size during updates.\n",
        "  \n",
        "  num_iters: The number of iterations to perform gradient descent.\n",
        "\n",
        "  Returns:\n",
        "      A tuple containing the learned weights and bias.\n",
        "\n"
      ],
      "metadata": {
        "id": "X1ei7CXIJbjf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hLghsfDhIQ8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a0cd081-70a7-40e2-942c-ff96c6d99a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned weights: [0.1561932  1.29286612]\n",
            "Learned bias: 0.7838639488436062\n"
          ]
        }
      ],
      "source": [
        "# what are the issue of learning rate high and learning rate low?\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def gradient_descent(X, y, learning_rate, num_iters):\n",
        "  # X = features; y = predicted value\n",
        "\n",
        "  # Initialize weights and bias with random values\n",
        "  m, n = X.shape\n",
        "  weights = np.random.randn(n)  # generating weight with random number\n",
        "  bias = 0\n",
        "\n",
        "  # Loop for the number of iterations\n",
        "  for i in range(num_iters):\n",
        "\n",
        "    # Predict y values using current weights and bias\n",
        "    y_predicted = np.dot(X, weights) + bias\n",
        "\n",
        "    # Calculate the error\n",
        "    error = y - y_predicted\n",
        "\n",
        "    # Calculate gradients for weights and bias\n",
        "    weight_gradient = -(2/m) * np.dot(X.T, error)\n",
        "    bias_gradient = -(2/m) * np.sum(error)\n",
        "\n",
        "    # Update weights and bias using learning rate\n",
        "    weights -= learning_rate * weight_gradient\n",
        "    bias -= learning_rate * bias_gradient\n",
        "\n",
        "  return weights, bias\n",
        "\n",
        "# Example usage\n",
        "X = np.array([[1,1], [2,2], [3,3]])\n",
        "y = np.array([2,4,5])\n",
        "learning_rate = 0.01\n",
        "num_iters = 100\n",
        "weights, bias = gradient_descent(X, y, learning_rate, num_iters)\n",
        "print(\"Learned weights:\", weights)\n",
        "print(\"Learned bias:\", bias)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_4mD-iJMsGkj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}